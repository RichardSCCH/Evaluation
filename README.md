# Evaluation

The evaluate.py script reads in the predicted and the reference summaries from each 
model and calculates the BLEU and ROUGE-L scores for each summary. The script prints
the average scores for each model and creates a csv file containing the scores of the 
models.

In the folder evaluation_result_exploration a jupyter notebook presents the results 
with diagrams and statistics of the model scores.